{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Satellite Images of Hurricane Damage\n",
    "\n",
    "\n",
    "**Overview**\n",
    "\n",
    "The data used in this notebook are satellite images from the Greater Houston area after Hurricane Harvey in 2017. Each image has been labelled as either \"Flooded/Damaged\" or \"Undamaged\".\n",
    "\n",
    "The data is available for download [here](https://www.kaggle.com/kmader/satellite-images-of-hurricane-damage), and the study associated with the dataset is available [here](https://arxiv.org/abs/1807.01688).\n",
    "\n",
    "**Motivation**\n",
    "\n",
    "As stated in the abstract of the associated study:\n",
    "\n",
    "> After a hurricane, damage assessment is critical to emergency managers for efficient response and resource allocation. One way to gauge the damage extent is to quantify the number of flooded/damaged buildings, which is traditionally done by ground survey. This process can be labor-intensive and time-consuming.\n",
    "\n",
    "One way to improve the efficiency of building damage assessment is to identify flooded/damaged buildings from satellite remote sensing data alone.\n",
    "\n",
    "**Method**\n",
    "\n",
    "In this notebook, I train a convolutional neural network to identify flooded/damaged buildings.\n",
    "\n",
    "In particular, I use transfer learning with fine-tuning to achieve high accuracy while minimizing the amount of compute required to train the classifier.\n",
    "\n",
    "Note that the notebook was executed using a Kaggle GPU kernel, so if you are viewing this notebook on GitHub, the images themselves have not been downloaded into the GitHub repository. However, you can still execute this notebook by either downloading the data using the links above, or forking this notebook on Kaggle [here](https://www.kaggle.com/yuempark/satellite-images-of-hurricane-damage).\n",
    "\n",
    "Also, as this was my first experience using TensorFlow and Keras, I found the following websites to be particularly helpful in getting the model up and running:\n",
    "\n",
    "* https://www.tensorflow.org/tutorials/load_data/images\n",
    "* https://www.tensorflow.org/tutorials/images/transfer_learning\n",
    "* https://www.tensorflow.org/guide/data"
   ]
  },
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input directory structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "!ls -lha ../input/satellite-images-of-hurricane-damage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../input/satellite-images-of-hurricane-damage/'\n",
    "\n",
    "def print_file_sizes(input_path, subset):\n",
    "    print('{}:'.format(subset))\n",
    "    print('')\n",
    "    path = input_path + subset + '/'\n",
    "    for f in os.listdir(path):\n",
    "        if not os.path.isdir(path + f):\n",
    "            print(f.ljust(30) + str(round(os.path.getsize(path + f) / 1000000, 2)) + 'MB')\n",
    "        else:\n",
    "            sizes = [os.path.getsize(path+f+'/'+x)/1000000 for x in os.listdir(path + f)]\n",
    "            print(f.ljust(30) + str(round(sum(sizes), 2)) + 'MB' + ' ({} files)'.format(len(sizes)))\n",
    "    print('')\n",
    "    \n",
    "print_file_sizes(input_path, 'train_another')\n",
    "print_file_sizes(input_path, 'validation_another')\n",
    "print_file_sizes(input_path, 'test_another')\n",
    "print_file_sizes(input_path, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Labels\n",
    "\n",
    "All labels for the images are encoded into the directory structure of this dataset. Just extract it into a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = pd.DataFrame({'path': list(Path(input_path).glob('**/*.jp*g'))})\n",
    "\n",
    "image_df['damage'] = image_df['path'].map(lambda x: x.parent.stem)\n",
    "image_df['data_split'] = image_df['path'].map(lambda x: x.parent.parent.stem)\n",
    "image_df['location'] = image_df['path'].map(lambda x: x.stem)\n",
    "image_df['lon'] = image_df['location'].map(lambda x: float(x.split('_')[0]))\n",
    "image_df['lat'] = image_df['location'].map(lambda x: float(x.split('_')[-1]))\n",
    "image_df['path'] = image_df['path'].map(lambda x: str(x)) # convert the path back to a string\n",
    "\n",
    "image_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a quick plot of the spatial distribution of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\n",
    "\n",
    "s = 10\n",
    "alpha = 0.5\n",
    "\n",
    "# get the train-validation-test splits\n",
    "image_df_train = image_df[image_df['data_split']=='train_another'].copy()\n",
    "image_df_val = image_df[image_df['data_split']=='validation_another'].copy()\n",
    "image_df_test = image_df[image_df['data_split']=='test_another'].copy()\n",
    "\n",
    "# sort to ensure reproducible behaviour\n",
    "image_df_train.sort_values('lat', inplace=True)\n",
    "image_df_val.sort_values('lat', inplace=True)\n",
    "image_df_test.sort_values('lat', inplace=True)\n",
    "image_df_train.reset_index(drop=True,inplace=True)\n",
    "image_df_val.reset_index(drop=True,inplace=True)\n",
    "image_df_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "ax[0].scatter(image_df_train['lon'], image_df_train['lat'], color='C0', s=s, alpha=alpha, label='train')\n",
    "ax[0].scatter(image_df_val['lon'], image_df_val['lat'], color='C1', s=s, alpha=alpha, label='validation')\n",
    "\n",
    "ax[0].set_title('Split', fontweight='bold')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Longitude', fontweight='bold')\n",
    "ax[0].set_ylabel('Latitude', fontweight='bold')\n",
    "\n",
    "image_df_dmg = image_df[image_df['damage']=='damage'].copy()\n",
    "image_df_nodmg = image_df[image_df['damage']=='no_damage'].copy()\n",
    "\n",
    "image_df_dmg.reset_index(drop=True,inplace=True)\n",
    "image_df_nodmg.reset_index(drop=True,inplace=True)\n",
    "\n",
    "ax[1].scatter(image_df_dmg['lon'], image_df_dmg['lat'], color='C0', s=s, alpha=alpha, label='damage')\n",
    "ax[1].scatter(image_df_nodmg['lon'], image_df_nodmg['lat'], color='C1', s=s, alpha=alpha, label='no damage')\n",
    "\n",
    "ax[1].set_title('Label', fontweight='bold')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Longitude', fontweight='bold')\n",
    "ax[1].set_ylabel('Latitude', fontweight='bold')\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Images\n",
    "\n",
    "First, inspect the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# read it in unchanged, to make sure we aren't losing any information\n",
    "img = cv2.imread(image_df['path'][0], cv2.IMREAD_UNCHANGED)\n",
    "np.shape(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(img[:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(img[:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a few (note that OpenCV reads the image in as BGR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, ncols=10, sharex=True, sharey=True, figsize=(20,10))\n",
    "\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(20):\n",
    "    img = cv2.imread(image_df_dmg['path'][i], cv2.IMREAD_UNCHANGED)\n",
    "    ax[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    ax[i].set_title('damage')\n",
    "    \n",
    "for i in range(20,40):\n",
    "    img = cv2.imread(image_df_nodmg['path'][i], cv2.IMREAD_UNCHANGED)\n",
    "    ax[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    ax[i].set_title('no damage')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpg_channels = ['blue','green','red']\n",
    "jpg_channel_colors = ['b','g','r']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "for i in range(len(jpg_channels)):\n",
    "    ax.hist(img[:,:,i].flatten(), bins=np.arange(256),\n",
    "            label=jpg_channels[i], color=jpg_channel_colors[i], alpha=0.5)\n",
    "    ax.legend()\n",
    "    \n",
    "ax.set_xlim(0,255)\n",
    "    \n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "In general, there are two types of transfer learning in the context of deep learning:\n",
    "\n",
    "* via feature extraction\n",
    "    * treating the network as an arbitrary feature extractor\n",
    "    * essentially 'chop off' the network at our pre-specified layer (typically prior to the fully-connected layers when actual classification predictions are made), then propagate some input through this 'shortened' network, get the output array, flatten it, and use that as the feature vector for the original input in another classification algorithm\n",
    "    * the two most common machine learning models for transfer learning via feature extraction are logistic regression and linear SVM:\n",
    "        * CNN's are non-linear models capable of learning non-linear features — we are assuming that the features learned by the CNN are already robust and discriminative\n",
    "        * feature vectors tend to be very large and have high dimensionality - we therefore need a fast model that can be trained on top of the features\n",
    "        * linear models tend to be very fast to train\n",
    "* via fine-tuning\n",
    "    * removing the fully-connected layers of an existing network, placing a new set of fully-connected layers on top of the network, and then fine-tuning these weights (and optionally previous layers) to recognize the new object classes\n",
    "    * this technique is usually recommended when the training dataset is large and very similar to the original dataset that the pre-trained model was trained on\n",
    "\n",
    "In this notebook, I will use the fine-tuning method, but an example of using the feature extraction method can be found [here](https://www.kaggle.com/kmader/damage-classification-with-resnet-features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "We could use `tf.keras.preprocessing` to load the images, but it has 3 downsides:\n",
    "\n",
    "1. it's slow\n",
    "2. it lacks fine-grained control\n",
    "3. it's not well integrated with the rest of TensorFlow\n",
    "\n",
    "Alternatively, we could use `tf.data.Dataset`. From the documentation:\n",
    "\n",
    "> The `tf.data` API enables you to build complex input pipelines from simple, reusable pieces. For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training.\n",
    "\n",
    "This API allows the user to only call the data (and apply transformations, etc.) when the data is actually needed, instead of keeping all the images in RAM indefinitely.\n",
    "\n",
    "I'll use the `tf.data.Dataset` API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the paths and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "train_path = image_df_train['path'].copy().values\n",
    "val_path = image_df_val['path'].copy().values\n",
    "test_path = image_df_test['path'].copy().values\n",
    "\n",
    "# labels\n",
    "train_labels = np.zeros(len(image_df_train), dtype=np.int8)\n",
    "train_labels[image_df_train['damage'].values=='damage'] = 1\n",
    "\n",
    "val_labels = np.zeros(len(image_df_val), dtype=np.int8)\n",
    "val_labels[image_df_val['damage'].values=='damage'] = 1\n",
    "\n",
    "test_labels = np.zeros(len(image_df_test), dtype=np.int8)\n",
    "test_labels[image_df_test['damage'].values=='damage'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge into a Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_path, train_labels))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_path, val_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_path, test_labels))\n",
    "\n",
    "# note that the `numpy()` function is required to grab the actual values from the Dataset\n",
    "for path, label in train_ds.take(5):\n",
    "    print(\"path  : \", path.numpy().decode('utf-8'))\n",
    "    print(\"label : \", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to get the actual images from the paths and do simple transformations. Then map these onto the Dataset:\n",
    "\n",
    "Note that this is where the most issues were experienced. The code here, which seems simple enough, will fail with an opaque error message even if the slightest thing is done incorrectly. Take note of the comments carefully..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function wraps `cv2.imread` - we treat it as a 'standalone' function, and therefore can use\n",
    "# eager execution (i.e. the use of `numpy()`) to get a string of the path.\n",
    "# note that no tensorflow functions are used here\n",
    "def cv2_imread(path, label):\n",
    "    # read in the image, getting the string of the path via eager execution\n",
    "    img = cv2.imread(path.numpy().decode('utf-8'), cv2.IMREAD_UNCHANGED)\n",
    "    # change from BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img, label\n",
    "\n",
    "# this function assumes that the image has been read in, and does some transformations on it\n",
    "# note that only tensorflow functions are used here\n",
    "def tf_cleanup(img, label):\n",
    "    # convert to Tensor\n",
    "    img = tf.convert_to_tensor(img)\n",
    "    # unclear why, but the jpeg is read in as uint16 - convert to uint8\n",
    "    img = tf.dtypes.cast(img, tf.uint8)\n",
    "    # set the shape of the Tensor\n",
    "    img.set_shape((128, 128, 3))\n",
    "    # convert to float32, scaling from uint8 (0-255) to float32 (0-1)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image\n",
    "    img = tf.image.resize(img, [128, 128])\n",
    "    # convert the labels into a Tensor and set the shape\n",
    "    label = tf.convert_to_tensor(label)\n",
    "    label.set_shape(())\n",
    "    return img, label\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# map the cv2 wrapper function using `tf.py_function`\n",
    "train_ds = train_ds.map(lambda path, label: tuple(tf.py_function(cv2_imread, [path, label], [tf.uint16, label.dtype])),\n",
    "                        num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.map(lambda path, label: tuple(tf.py_function(cv2_imread, [path, label], [tf.uint16, label.dtype])),\n",
    "                    num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.map(lambda path, label: tuple(tf.py_function(cv2_imread, [path, label], [tf.uint16, label.dtype])),\n",
    "                      num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# map the TensorFlow transformation function - no need to wrap\n",
    "train_ds = train_ds.map(tf_cleanup, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.map(tf_cleanup, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.map(tf_cleanup, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image augmentation:\n",
    "\n",
    "Augment the training and validation data by applying random flips and rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_augmentation(img, label):\n",
    "    # rotate 0, 90, 180, or 270 degrees with 25% probability for each\n",
    "    img = tf.image.rot90(img, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32, seed=1111))\n",
    "    return img, label\n",
    "\n",
    "def flip_augmentation(img, label):\n",
    "    # flip with 50% probability for left-right and up-down\n",
    "    img = tf.image.random_flip_left_right(img, seed=2222)\n",
    "    img = tf.image.random_flip_up_down(img, seed=3333)\n",
    "    return img, label\n",
    "\n",
    "# map the augmentations, creating a new Dataset\n",
    "augmented_train_ds = train_ds.map(rotate_augmentation, num_parallel_calls=AUTOTUNE)\n",
    "augmented_train_ds = augmented_train_ds.map(flip_augmentation, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "augmented_val_ds = val_ds.map(rotate_augmentation, num_parallel_calls=AUTOTUNE)\n",
    "augmented_val_ds = augmented_val_ds.map(flip_augmentation, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# concatenate the augmented and original datasets\n",
    "train_ds = train_ds.concatenate(augmented_train_ds)\n",
    "val_ds = val_ds.concatenate(augmented_val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle:\n",
    "\n",
    "Note that the buffer size for shuffling defines how random the Dataset becomes - a buffer size that's equal to the number of instances will result in a uniform shuffling over the entire Dataset, and a buffer size equal to 1 will result in no shuffling. Since the data is currently ordered by label, we need make sure we do a nice full shuffle over the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double the number of samples in the training and validation splits, due to our augmentation procedure\n",
    "n_train = len(train_labels)*2\n",
    "n_val = len(val_labels)*2\n",
    "n_test = len(test_labels)\n",
    "\n",
    "# shuffle over the entire dataset, seeding the shuffling for reproducible results\n",
    "train_ds = train_ds.shuffle(n_train, seed=2019, reshuffle_each_iteration=True)\n",
    "val_ds = val_ds.shuffle(n_val, seed=2019, reshuffle_each_iteration=True)\n",
    "test_ds = test_ds.shuffle(n_test, seed=2019, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure everything was read in correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_check = 0\n",
    "for element in train_ds:\n",
    "    n_train_check = n_train_check + 1\n",
    "print(n_train_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val_check = 0\n",
    "for element in val_ds:\n",
    "    n_val_check = n_val_check + 1\n",
    "print(n_val_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_check = 0\n",
    "for element in test_ds:\n",
    "    n_test_check = n_test_check + 1\n",
    "print(n_test_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the image was read in correctly\n",
    "for image, label in train_ds.take(1):\n",
    "    print(\"image shape : \", image.numpy().shape)\n",
    "    print(\"label       : \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, ncols=10, sharex=True, sharey=True, figsize=(20,10))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for image, label in train_ds.take(10):\n",
    "    ax[0,i].imshow(image[:,:,0])\n",
    "    ax[0,i].set_title('{} - {}'.format(label.numpy(), 'R'))\n",
    "    ax[1,i].imshow(image[:,:,1])\n",
    "    ax[1,i].set_title('{} - {}'.format(label.numpy(), 'G'))\n",
    "    ax[2,i].imshow(image[:,:,2])\n",
    "    ax[2,i].set_title('{} - {}'.format(label.numpy(), 'B'))\n",
    "    ax[3,i].imshow(image)\n",
    "    ax[3,i].set_title('{} - {}'.format(label.numpy(), 'RGB'))\n",
    "    \n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=3, sharex=True, sharey=True, figsize=(20,15))\n",
    "\n",
    "ax[0,0].set_xlim(0,1)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for image, label in train_ds.take(3):\n",
    "    ax[i,0].hist(image[:,:,0].numpy().flatten())\n",
    "    ax[i,0].set_title('{} - {}'.format(label.numpy(), 'R'))\n",
    "    ax[i,1].hist(image[:,:,1].numpy().flatten())\n",
    "    ax[i,1].set_title('{} - {}'.format(label.numpy(), 'G'))\n",
    "    ax[i,2].hist(image[:,:,2].numpy().flatten())\n",
    "    ax[i,2].set_title('{} - {}'.format(label.numpy(), 'B'))\n",
    "    \n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch the data. A batch size of 32 seems like a common starting point, and using powers of 2 is preferred when using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_batches_ds = train_ds.batch(BATCH_SIZE)\n",
    "val_batches_ds = val_ds.batch(BATCH_SIZE)\n",
    "test_batches_ds = test_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect a batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, label_batch in train_batches_ds.take(1):\n",
    "    print(image_batch.shape)\n",
    "    print(label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "Let's use VGG16 for the base of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (128, 128, 3)\n",
    "\n",
    "# create the base model from the pre-trained model VGG16\n",
    "# note that, if using a Kaggle server, internet has to be turned on\n",
    "pretrained_model = tf.keras.applications.vgg16.VGG16(input_shape=IMG_SHAPE,\n",
    "                                                     include_top=False,\n",
    "                                                     weights='imagenet')\n",
    "\n",
    "\n",
    "# freeze the convolutional base\n",
    "pretrained_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pretrained_model` converts each 128x128x3 image to a 4x4x512 block of features. See what it does to the example batch of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_batch = pretrained_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average over the 4x4 spatial locations to convert the block of features into a single 512-element vector per image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a `tf.keras.layers.Dense` layer to convert these features into a single prediction per image. You don't need an activation function here because this prediction will be treated as a logit, or a raw prediction value. Positive numbers predict class 1, negative numbers predict class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the initializers with a seed for reproducible behaviour\n",
    "prediction_layer = tf.keras.layers.Dense(1,\n",
    "                                         kernel_initializer=tf.keras.initializers.GlorotUniform(seed=1992),\n",
    "                                         bias_initializer=tf.keras.initializers.GlorotUniform(seed=1992))\n",
    "\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_base = tf.keras.applications.vgg16.VGG16(input_shape=(128, 128, 3),\n",
    "#                                                      include_top=False,\n",
    "#                                                      weights='imagenet')\n",
    "# cnn_out = tf.keras.layers.GlobalAveragePooling2D()(cnn_base.output)\n",
    "# cnn = tf.keras.Model(cnn_base.input, cnn_out)\n",
    "# cnn.trainable = False\n",
    "# encoded_frames = tf.keras.layers.TimeDistributed(cnn, input_shape=(None, 10, 128, 128, 3))\n",
    "# encoded_sequence = tf.keras.layers.LSTM(256, encoded_frames)\n",
    "\n",
    "\n",
    "# model = tf.keras.Sequential([cnn,\n",
    "#                              encoded_frames,\n",
    "#                              encoded_sequence,\n",
    "#                              prediction_layer]\n",
    "#                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now stack all the components using a `tf.keras.Sequential` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([pretrained_model,\n",
    "                             global_average_layer,\n",
    "                             prediction_layer]\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model. Since there are two classes, use a binary cross-entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 15\n",
    "steps_per_epoch = n_train//BATCH_SIZE\n",
    "validation_steps = 20\n",
    "\n",
    "loss0, accuracy0 = model.evaluate(val_batches_ds, steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_batches_ds,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=val_batches_ds,\n",
    "                    validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10,8), sharex=True)\n",
    "\n",
    "x_plot = np.arange(1, initial_epochs+1)\n",
    "\n",
    "ax[0].plot(x_plot, acc, '+-', label='Training')\n",
    "ax[0].plot(x_plot, val_acc, '+-', label='Validation')\n",
    "ax[0].legend()\n",
    "ax[0].set_ylabel('Accuracy', fontweight='bold')\n",
    "ax[0].set_ylim(0.5, 1)\n",
    "# ax[0].grid(ls='--', c='C7')\n",
    "ax[0].set_title('Accuracy', fontweight='bold')\n",
    "\n",
    "ax[1].plot(x_plot, loss, '+-', label='Training')\n",
    "ax[1].plot(x_plot, val_loss, '+-', label='Validation')\n",
    "ax[1].legend()\n",
    "ax[1].set_ylabel('cross entropy', fontweight='bold')\n",
    "ax[1].set_ylim(0, 1)\n",
    "# ax[1].grid(ls='--', c='C7')\n",
    "ax[1].set_title('loss', fontweight='bold')\n",
    "ax[1].set_xlabel('epoch', fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_loss, val_accuracy = model.evaluate(val_batches_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_batches_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes from the TensorFlow tutorial on transfer learning:\n",
    "\n",
    "> Now fine-tune the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset. This should only be attempted after you have trained the top-level classifier with the pre-trained model set to non-trainable. If you add a randomly initialized classifier on top of a pre-trained model and attempt to train all layers jointly, the magnitude of the gradient updates will be too large (due to the random weights from the classifier) and your pre-trained model will forget what it has learned. Only a small number of top layers of the pre-trained model should be fine-tuned. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features which generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze the layers\n",
    "pretrained_model.trainable = True\n",
    "\n",
    "# let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the pre-trained model: \", len(pretrained_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune from this layer onwards\n",
    "fine_tune_at = 67\n",
    "\n",
    "# freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in pretrained_model.layers[:fine_tune_at]:\n",
    "  layer.trainable =  False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recompile the model using a lower training rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate/10),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 30\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_batches_ds,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1]+1,\n",
    "                         validation_data=val_batches_ds,\n",
    "                         validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated training curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy'][:15] + history_fine.history['accuracy']\n",
    "val_acc = history.history['val_accuracy'][:15] + history_fine.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss'][:15] + history_fine.history['loss']\n",
    "val_loss = history.history['val_loss'][:15] + history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc += history_fine.history['accuracy']\n",
    "# val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "# loss += history_fine.history['loss']\n",
    "# val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10,8), sharex=True)\n",
    "\n",
    "x_plot = np.arange(1, 46)\n",
    "\n",
    "ax[0].plot(x_plot, acc, '+-', label='training')\n",
    "ax[0].plot(x_plot, val_acc, '+-', label='validation')\n",
    "ax[0].legend()\n",
    "ax[0].set_ylabel('accuracy', fontweight='bold')\n",
    "ax[0].set_ylim(0.5, 1)\n",
    "# ax[0].grid(ls='--', c='C7')\n",
    "ax[0].set_title('accuracy', fontweight='bold')\n",
    "# ax[0].axvline(initial_epochs, c='C7', ls='--')\n",
    "\n",
    "ax[1].plot(x_plot, loss, '+-', label='training')\n",
    "ax[1].plot(x_plot, val_loss, '+-', label='validation')\n",
    "ax[1].legend()\n",
    "ax[1].set_ylabel('cross entropy', fontweight='bold')\n",
    "ax[1].set_ylim(0, 1)\n",
    "# ax[1].grid(ls='--', c='C7')\n",
    "ax[1].set_title('loss', fontweight='bold')\n",
    "ax[1].set_xlabel('epoch', fontweight='bold')\n",
    "# ax[1].axvline(initial_epochs, c='C7', ls='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a prediction on the full validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = model.evaluate(val_batches_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_batches_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the study associated with this dataset, the best performance achieved was:\n",
    "\n",
    "* convolutional neural network + data augmentation + 50% dropout in the fully connected layer + Adam optimizer:\n",
    "    * validation accuracy: 98.06%\n",
    "    * test accuracy (balanced): 97.29%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
