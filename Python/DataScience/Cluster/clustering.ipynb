{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter file for assignment on Clustering\n",
    "### Author: Tri Lam\n",
    "### UH ID: 1916079"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for MinMax feature normalization\n",
    "The input `x` is the raw data in a 2-D array of the shape `(number of data points, number of features`.\n",
    "\n",
    "The output `x_norm` is the normalized data of the input `x` with the same shape as the input.\n",
    "\n",
    "This function will be used for normalizing data before using DBSCAN for clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_norm(x):\n",
    "    # x is a 2-D array of the shape (number of data points, number of features\n",
    "    eps = np.finfo(float).eps\n",
    "    x_norm = x - np.expand_dims(x.min(0), axis=0)\n",
    "    x_norm = x_norm / (np.expand_dims((x.max(0) - x.min(0)), axis=0) + eps)\n",
    "    \n",
    "    return x_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
      "0  75.0        0                       582         0                 20   \n",
      "1  55.0        0                      7861         0                 38   \n",
      "2  65.0        0                       146         0                 20   \n",
      "3  50.0        1                       111         0                 20   \n",
      "4  65.0        1                       160         1                 20   \n",
      "\n",
      "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
      "0                    1  265000.00               1.9           130    1   \n",
      "1                    0  263358.03               1.1           136    1   \n",
      "2                    0  162000.00               1.3           129    1   \n",
      "3                    0  210000.00               1.9           137    1   \n",
      "4                    0  327000.00               2.7           116    0   \n",
      "\n",
      "   smoking  time  DEATH_EVENT  \n",
      "0        0     4            1  \n",
      "1        0     6            1  \n",
      "2        1     7            1  \n",
      "3        0     7            1  \n",
      "4        0     8            1  \n"
     ]
    }
   ],
   "source": [
    "data_path = 'clinical_records_dataset.csv'  # Make sure this path is correct\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows to verify loading\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Function for computing purity\n",
    "This is your function of purity.\n",
    "\n",
    "The indices of the clusters in `y_true` and `y_pred` start from 0 in `compute_purity`, e.g., [1, 1, 0, 0, 2, 2, 2].\n",
    "\n",
    "`y_true` is the array of true class indices of all data points, `len(y_true)=number of data points`.\n",
    "\n",
    "`y_pred` is the array of cluster indices of all data points, `len(y_pred)=number of data points`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_purity(y_true, y_pred):\n",
    "    # This is your function of purity\n",
    "    # y_true is the array of true class indices of all data points, len(y_true)=number of data points\n",
    "    # y_pred is the array of cluster indices of all data points, len(y_pred)=number of data points\n",
    "    total_points = len(y_true)\n",
    "    unique_clusters = np.unique(y_pred)\n",
    "    \n",
    "    majority_class_count = 0\n",
    "    for cluster in unique_clusters:\n",
    "        indices = np.where(y_pred == cluster)[0]\n",
    "        true_labels_in_cluster = y_true[indices]\n",
    "        class_counts = Counter(true_labels_in_cluster)\n",
    "        majority_class_count += max(class_counts.values())\n",
    "    \n",
    "    purity = majority_class_count / total_points\n",
    "    return purity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.75\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array([2, 2, 1, 2, 2, 2, 0, 0, 0, 1, 2, 1, 1, 1, 1, 1])\n",
    "y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])\n",
    "\n",
    "purity = compute_purity(y_true, y_pred)\n",
    "print(\"Purity:\", purity) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Run K-means on the dataset with k=2. Use the default parameters in the function provided by scikit-learn for the algorithm. What percentage of the data points were assigned to each of the two clusters? Compute the purity of the clustering result. Then compute the purity of the clustering result for each of the two clusters. Which cluster has the highest purity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Purity of K-Means clustering (k=2): 0.6789297658862876\n",
      "Percentage of data points in each cluster: {np.int32(0): 51.50501672240802, np.int32(1): 48.49498327759198}\n",
      "Purity for each cluster: {np.int32(0): 0.6558441558441559, np.int32(1): 0.7034482758620689}\n",
      "Cluster 1 has the highest purity with a score of 0.7034482758620689\n"
     ]
    }
   ],
   "source": [
    "# Extract features and target, then normalize the features\n",
    "X = data.drop(columns=['time', 'DEATH_EVENT']).values  # Use the exact column names\n",
    "y_true = data['DEATH_EVENT'].values  # Extract the target column\n",
    "\n",
    "X_normalized = feature_norm(X)\n",
    "\n",
    "# Run K-Means clustering with k=2\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X_normalized)\n",
    "\n",
    "# Calculate overall purity for the K-Means clustering\n",
    "overall_purity = compute_purity(y_true, y_pred)\n",
    "print(\"Overall Purity of K-Means clustering (k=2):\", overall_purity)\n",
    "\n",
    "# Calculate the percentage of data points in each cluster\n",
    "cluster_counts = Counter(y_pred)\n",
    "total_points = len(y_pred)\n",
    "cluster_percentages = {cluster: count / total_points * 100 for cluster, count in cluster_counts.items()}\n",
    "print(\"Percentage of data points in each cluster:\", cluster_percentages)\n",
    "\n",
    "# Calculate purity for each individual cluster\n",
    "cluster_purities = {}\n",
    "for cluster in cluster_counts:\n",
    "    indices = np.where(y_pred == cluster)[0]\n",
    "    true_labels_in_cluster = y_true[indices]\n",
    "    class_counts = Counter(true_labels_in_cluster)\n",
    "    cluster_purity = max(class_counts.values()) / len(true_labels_in_cluster)\n",
    "    cluster_purities[cluster] = cluster_purity\n",
    "\n",
    "print(\"Purity for each cluster:\", cluster_purities)\n",
    "\n",
    "# Identify which cluster has the highest purity\n",
    "highest_purity_cluster = max(cluster_purities, key=cluster_purities.get)\n",
    "print(f\"Cluster {highest_purity_cluster} has the highest purity with a score of {cluster_purities[highest_purity_cluster]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Run K-means on the dataset with k=2, 10, 30, 50, 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     k  avg_purity  avg_silhouette\n",
      "0    2    0.678930        0.197685\n",
      "1   10    0.678930        0.311660\n",
      "2   30    0.703679        0.472695\n",
      "3   50    0.733445        0.364854\n",
      "4  100    0.796321        0.232349\n",
      "Best k for purity: 100\n",
      "Best k for silhouette coefficient: 30\n"
     ]
    }
   ],
   "source": [
    "k_values = [2, 10, 30, 50, 100]\n",
    "results = []\n",
    "\n",
    "# Iterate over each value of k\n",
    "for k in k_values:\n",
    "    purities = []\n",
    "    silhouettes = []\n",
    "    \n",
    "    for run in range(10):  \n",
    "        kmeans = KMeans(n_clusters=k, random_state=run)\n",
    "        y_pred = kmeans.fit_predict(X_normalized)\n",
    "        \n",
    "        purity = compute_purity(y_true, y_pred)\n",
    "        purities.append(purity)\n",
    "        \n",
    "        silhouette = silhouette_score(X_normalized, y_pred, metric='euclidean')\n",
    "        silhouettes.append(silhouette)\n",
    "    \n",
    "    avg_purity = np.mean(purities)\n",
    "    avg_silhouette = np.mean(silhouettes)\n",
    "    results.append({'k': k, 'avg_purity': avg_purity, 'avg_silhouette': avg_silhouette})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "best_purity_k = results_df.loc[results_df['avg_purity'].idxmax(), 'k']\n",
    "best_silhouette_k = results_df.loc[results_df['avg_silhouette'].idxmax(), 'k']\n",
    "\n",
    "print(f\"Best k for purity: {best_purity_k}\")\n",
    "print(f\"Best k for silhouette coefficient: {best_silhouette_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "First, apply MinMax normalization we have used in our first EDA assignment on this dataset (a normalization function \"feature_norm\" has been included in the shell python notebook).\n",
    "\n",
    "Then run DBSCAN on the normalized data with eps=0.3, 0.5, 0.7, and fix minPts=5, metric=Euclidean distance, and other default parameter values. Count the total number of clusters and the total number of anomalies generated by DBSCAN, calculate the purity of the clustering, and generate a table, as given below. Which value of eps gives the best clustering result in terms of purity?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eps  Number of Clusters  Number of Anomalies    Purity\n",
      "0  0.3                  18                  146  0.688963\n",
      "1  0.5                  22                   21  0.688963\n",
      "2  0.7                  22                   13  0.695652\n"
     ]
    }
   ],
   "source": [
    "eps_values = [0.3, 0.5, 0.7]  # Different epsilon values to test\n",
    "task_4_results = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=5, metric='euclidean')  \n",
    "    y_pred = dbscan.fit_predict(X_normalized)  \n",
    "\n",
    "    n_clusters = len(set(y_pred)) - (1 if -1 in y_pred else 0)\n",
    "\n",
    "    n_anomalies = list(y_pred).count(-1)\n",
    "\n",
    "    purity = compute_purity(y_true, y_pred) if n_clusters > 0 else 0\n",
    "\n",
    "    task_4_results.append({\n",
    "        'eps': eps,\n",
    "        'Number of Clusters': n_clusters,\n",
    "        'Number of Anomalies': n_anomalies,\n",
    "        'Purity': purity\n",
    "    })\n",
    "\n",
    "task_4_results_df = pd.DataFrame(task_4_results)\n",
    "print(task_4_results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
